# -*- coding: utf-8 -*-
"""mnist.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10x-GvCp6vRToPHo7AF3zrgq4oMOAs9_F
"""

pip install imblearn

import tensorflow as tf
import numpy as np
from imblearn.over_sampling import SMOTE
from imblearn.under_sampling import NearMiss
from sklearn.linear_model import LinearRegression
from sklearn import svm
from sklearn.naive_bayes import GaussianNB
from sklearn.ensemble import RandomForestClassifier
from sklearn.datasets import make_classification
from sklearn.ensemble import RandomForestClassifier
import seaborn as sns; sns.set()
from sklearn.model_selection import cross_val_score
from sklearn.ensemble import BaggingClassifier, ExtraTreesClassifier, RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import RidgeClassifier
from sklearn.ensemble import VotingClassifier

"""READING THE DATA"""

(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()

"""MANIPULATING THE DATA"""

x_train_mod = []
for i in range(0,len(x_train)):
  A = np.array(x_train[i]).ravel()
    # x_train[i] = x_train[i].ravel()
  # print(A.shape)
  x_train_mod.append(A)

x_test_mod = []
for i in range(0,len(x_test)):
  A = np.array(x_test[i]).ravel()
    # x_train[i] = x_train[i].ravel()
  # print(A.shape)
  x_test_mod.append(A)

"""CREATING IMBALANCE"""

for i in range (0,len(y_train)):
  if y_train[i] == 2:
    y_train[i]=0
  else:
    y_train[i]=1
for i in range (0,len(y_test)):
  if y_test[i] == 2:
    y_test[i]=0
  else:
    y_test[i]=1

"""COUNT OF ORIGINAL DATA"""

class_zero = 0
class_one = 0
for i in range(0,len(y_train)):
  if y_train[i]==0:
    class_zero = class_zero+1
  else:
    class_one = class_one + 1
print(class_zero, class_one)

"""OVERSAMPLING"""

smt = SMOTE()
over_x_train, over_y_train = smt.fit_sample(x_train_mod, y_train)
np.bincount(over_y_train)

"""UNDERSAMPLING"""

nr = NearMiss()
under_x_train, under_y_train = nr.fit_sample(x_train_mod, y_train)
np.bincount(under_y_train)

"""CLASSIFIER"""

def acc(pred, actual):
  tp = fp = tn = fn = 0
  for i in range(0, len(pred)):
    if np.round(pred[i]) == actual[i]:
      if np.round(pred[i])==0:
        tp=tp+1
      else:
        tn=tn+1
    else:
      if np.round(pred[i])==0:
        fp = fp+1
      else:
        fn=fn+1
  return tp, fp, tn, fn

def calculate_accuracy(y_pred_reg, y_pred_sv, y_pred_nb, y_pred_rf, y_test):
  print('For Linear Regression')
  tp, fp, tn, fn = acc(y_pred_reg, y_test)
  print('Accuracy : ', (tp+tn)/(tp+tn+fp+fn))
  conf_reg = [[tp , fp],[fn , tn]]
  reg = sns.heatmap(conf_reg, annot=True)   
  print(reg) 

  print('For SVM')
  tp, fp, tn, fn = acc(y_pred_sv, y_test)
  print('Accuracy : ', (tp+tn)/(tp+tn+fp+fn))
  conf_sv = [[tp , fp],[fn , tn]]
  sv = sns.heatmap(conf_sv, annot=True)   
  print(sv) 

  print('For Naive Bayes')
  tp, fp, tn, fn = acc(y_pred_nb, y_test)
  print('Accuracy : ', (tp+tn)/(tp+tn+fp+fn))
  conf_nb = [[tp , fp],[fn , tn]]
  nb = sns.heatmap(conf_nb, annot=True)   
  print(nb) 

  print('For Random Forest')
  tp, fp, tn, fn = acc(y_pred_rf, y_test)
  print('Accuracy : ', (tp+tn)/(tp+tn+fp+fn))
  conf_rf = [[tp , fp],[fn , tn]]
  rf = sns.heatmap(conf_rf, annot=True)   
  print(rf)

def classify_all(x_train, y_train, x_test, y_test):
  # Linear Regression
  reg = LinearRegression().fit(x_train, y_train)
  y_pred_reg = reg.predict(x_test)
  print('For Linear Regression')
  tp, fp, tn, fn = acc(y_pred_reg, y_test)
  print('Accuracy : ', (tp+tn)/(tp+tn+fp+fn))
  conf_reg = [[tp , fp],[fn , tn]]
  reg = sns.heatmap(conf_reg, annot=True)   
  print(conf_reg)
  # print(reg)

  # SVM
  sv = svm.SVC(gamma='scale')
  sv.fit(x_train, y_train)
  y_pred_sv = sv.predict(x_test)
  print('For SVM')
  tp, fp, tn, fn = acc(y_pred_sv, y_test)
  print('Accuracy : ', (tp+tn)/(tp+tn+fp+fn))
  conf_sv = [[tp , fp],[fn , tn]]
  sv = sns.heatmap(conf_sv, annot=True)   
  print(conf_sv)
  # print(sv) 

  # Naive Bayes
  gnb = GaussianNB().fit(x_train, y_train)
  y_pred_nb = gnb.predict(x_test)
  print('For Naive Bayes')
  tp, fp, tn, fn = acc(y_pred_nb, y_test)
  print('Accuracy : ', (tp+tn)/(tp+tn+fp+fn))
  conf_nb = [[tp , fp],[fn , tn]]
  nb = sns.heatmap(conf_nb, annot=True) 
  print(conf_nb)  
  # print(nb) 

  # Random Forest
  rf = RandomForestClassifier(n_estimators=20, random_state=0)
  rf.fit(x_train, y_train)
  y_pred_rf = rf.predict(x_test)
  print('For Random Forest')
  tp, fp, tn, fn = acc(y_pred_rf, y_test)
  print('Accuracy : ', (tp+tn)/(tp+tn+fp+fn))
  conf_rf = [[tp , fp],[fn , tn]]
  rf = sns.heatmap(conf_rf, annot=True) 
  print(conf_rf)  
  # print(rf) 

  return (reg, sv, nb, rf)

  # calculate_accuracy(y_pred_reg, y_pred_sv, y_pred_nb, y_pred_rf, y_test)

"""Main Calling"""

# For over sampling
print('For Over Sampling')
(reg, sv, nb, rf) = classify_all(over_x_train, over_y_train, x_test_mod, y_test)


# For under sampling
print('For Under Sampling')
(reg1, sv1, nb1, rf1) = classify_all(under_x_train, under_y_train, x_test_mod, y_test)

a = sns.heatmap([[1028, 5676], [4, 3292]], cmap="YlGnBu", annot=True, fmt="d")
# print(a)

"""EMSEMBLE : BOOSTING"""

from sklearn.metrics import confusion_matrix

rf = RandomForestClassifier()
reg = LinearRegression()
gnb = GaussianNB()
svc = svm.SVC(gamma='scale')

base_methods=[rf, reg, gnb, svc]
for bm  in base_methods:
 print("Method: ", bm)
 bag_model=BaggingClassifier(base_estimator=bm,n_estimators=100,bootstrap=True)
 bag_model=bag_model.fit(over_x_train,over_y_train)
 ytest_pred=bag_model.predict(x_test_mod)
 print(bag_model.score(x_test_mod, y_test))
 print(confusion_matrix(y_test, ytest_pred))

# clf_array = [rf, reg, gnb, svc]
# for clf in clf_array:
#     vanilla_scores = cross_val_score(clf, over_x_train, y_train, cv=10, n_jobs=-1)
#     bagging_clf = BaggingClassifier(clf, max_samples=0.4, max_features=10, random_state=seed)
#     bagging_scores = cross_val_score(bagging_clf, x_train_mod, y_train, cv=10, n_jobs=-1)
# clf = [rf, reg, gnb, svc]
# eclf = VotingClassifier(estimators=[('Random Forests', rf), ('Logistic Rgression', reg), ('Naive Bayes', gnb), ('SVM', svc)], voting='hard')
# for clf, label in zip([rf, reg, gnb, svc], ['Random Forest', 'Logistic Regression', 'Naive Bayes', 'SVM']):
#     scores = cross_val_score(clf, over_x_train, y_train, cv=10, scoring='accuracy')
#     print("Accuracy: %0.2f (+/- %0.2f) [%s]" % (scores.mean(), scores.std(), label))

rf = RandomForestClassifier()
reg = LinearRegression()
gnb = GaussianNB()
svc = svm.SVC(gamma='scale')

base_methods=[rf, reg, gnb, svc]
for bm  in base_methods:
 print("Method: ", bm)
 bag_model=BaggingClassifier(base_estimator=bm,n_estimators=100,bootstrap=True)
 bag_model=bag_model.fit(under_x_train,under_y_train)
 ytest_pred=bag_model.predict(x_test)
 print(bag_model.score(x_test, y_test))
 print(confusion_matrix(y_test, ytest_pred))

"""BOOSTING"""

from sklearn.ensemble import GradientBoostingClassifier
gb_clf = GradientBoostingClassifier(n_estimators=20, learning_rate=0.75, max_features=2, max_depth=2, random_state=0)
gb_clf.fit(over_x_train, over_y_train)

# print("Learning rate: ", learning_rate)
# print("Accuracy score (training): {0:.3f}".format(gb_clf.score(X_t, y_t)))
# print("Accuracy score (validation): {0:.3f}".format(gb_clf.score(X_test, y_test)))
y_pred_gradient_boosting_ov=gb_clf.predict(x_test_mod)
tp, fp, tn, fn = acc(y_pred_gradient_boosting_ov, y_test)
print('Accuracy ', (tp+tn)/(tp+tn+fp+fn))
print(tp, fp, fn, tn)

gb_clf = GradientBoostingClassifier(n_estimators=20, learning_rate=0.75, max_features=2, max_depth=2, random_state=0)
gb_clf.fit(under_x_train, under_y_train)

# print("Learning rate: ", learning_rate)
# print("Accuracy score (training): {0:.3f}".format(gb_clf.score(X_t, y_t)))
# print("Accuracy score (validation): {0:.3f}".format(gb_clf.score(X_test, y_test)))
y_pred_gradient_boosting_ov=gb_clf.predict(x_test_mod)
tp, fp, tn, fn = acc(y_pred_gradient_boosting_ov, y_test)
print('Accuracy ', (tp+tn)/(tp+tn+fp+fn))
print(tp, fp, fn, tn)

a = sns.heatmap([[839, 624], [193 ,8344]], cmap="YlGnBu", annot=True, fmt="d")
# print(a)

a = sns.heatmap([[993, 5882], [39, 3086]], cmap="YlGnBu", annot=True, fmt="d")
# print(a)

